# Ground-up ML Reboot

**A hands-on machine learning reboot â€” from first principles to production-ready benchmarking and diagnostics.**

This project is a structured, from-scratch journey through core machine learning topics.
Each mini-project includes multiple implementations to help build understanding and transition from intuition to industrial-strength workflows.

---

## ğŸ” Whatâ€™s Inside

Each mini-project demonstrates the same ML concept in three ways:

- âœ… **Pure Python/Numpy** â€” for building foundational intuition.
- âš™ï¸ **Scikit-learn** â€” for standard modeling pipelines.
- ğŸ”¥ **PyTorch** â€” for scalable, deep learning-friendly extensions.

Projects include:

```
src/
â”œâ”€â”€ simple_linear_regression/
â”‚   â”œâ”€â”€ from_scratch/     # Linear regression built manually with NumPy
â”‚   â”œâ”€â”€ sklearn_impl/     # Linear regression using scikit-learn
â”‚   â”œâ”€â”€ pytorch_impl/     # Linear regression using PyTorch
â”‚   â””â”€â”€ notebook.ipynb    # Visual + code comparison
...
```

Shared folders for `data/` and `shared_utils/` allow easy reuse of common loaders, metrics, and plotting functions across implementations.

---

## ğŸš€ Current Status (v1.0.0)

**Simple Linear Regression Fully Implemented:**

- Five training methods supported:
  - Beta estimations
  - Normal equation
  - Batch gradient descent
  - Stochastic gradient descent
  - Mini-batch gradient descent
- Benchmarking runner to compare models/methods dynamically
- Cost history tracking and convergence visualizations
- Unified APIs for easy extension
- Extensible dispatch system for future models

---

## ğŸ›  Key Features

- ğŸ“ˆ Benchmark model performance (RMSE, MAE, RÂ²) across implementations
- ğŸ§  Understand optimizer behaviors through loss curves
- ğŸ”€ Flexibly specify any model/method combination to compare
- ğŸ—ï¸ Access trained models programmatically for further diagnostics
- ğŸ“š Built for education, research, and real-world ML pipeline foundations

---

## ğŸ“š Vision for Future Work

- Multivariate Linear Regression benchmarking
- Logistic Regression with gradient descent solvers
- Model assumption validation module integration (homoscedasticity, normality, etc.)
- Batch size, learning rate, and epoch tuning sweeps

---

## ğŸ§  Who This Is For

- Developers rebuilding their ML foundation with greater rigor
- Educators seeking clean, step-by-step ML concept demos
- Practitioners needing side-by-side framework benchmarks
- Researchers profiling computational trade-offs between frameworks

---

## ğŸš€ Getting Started

```bash
# Clone the repository
git clone https://github.com/w-thurston/ground-up-ml.git
cd ground-up-ml

# Set up environment (using poetry or pip)
pip install -r requirements.txt
# or
poetry install
```

Then dive into any `notebook.ipynb` in `src/` to see side-by-side comparisons and results!

---

## ğŸ“š License

MIT License. Use, remix, or contribute!

---

_This project is part of an educational blog series, **Ground-up ML Reboot** â€” teaching machines (and ourselves) how to learn from scratch._
